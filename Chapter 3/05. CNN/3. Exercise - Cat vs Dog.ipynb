{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prescription-footwear",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\"> Cats vs Dogs </h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\"> Loading your dataset</h2>\n",
    "<div>\n",
    "\n",
    "<br>\n",
    "    \n",
    "So far we've been working with fairly artificial datasets that you wouldn't typically be using in real projects. Instead, you'll likely be dealing with full-sized images like you'd get from smart phone cameras. In this notebook, we'll look at how to load images and use them to train neural networks.\n",
    "\n",
    "We'll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n",
    "\n",
    "<img src=\"imgs/dog.png\" width=\"200\" height=\"40\" />\n",
    "<img src=\"imgs/cat.png\" width=\"200\" height=\"40\" />\n",
    "    \n",
    "\n",
    "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "Start importing the needed libraries:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-spiritual",
   "metadata": {},
   "source": [
    "The easiest way to load image data is with `datasets.ImageFolder` from `torchvision` ([documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder)). In general you'll use `ImageFolder` like so:\n",
    "\n",
    "```python\n",
    "dataset = datasets.ImageFolder('path/to/data', transform=transform)\n",
    "```\n",
    "\n",
    "where `'path/to/data'` is the file path to the data directory and `transform` is a list of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. ImageFolder expects the files and directories to be constructed like so:\n",
    "```\n",
    "root/dog/xxx.png\n",
    "root/dog/xxy.png\n",
    "root/dog/xxz.png\n",
    "\n",
    "root/cat/123.png\n",
    "root/cat/nsdf3.png\n",
    "root/cat/asd932_.png\n",
    "```\n",
    "\n",
    "where each class has it's own directory (`cat` and `dog`) for the images. The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. You can download the dataset already structured like this [from here](https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip). They are already splitted into a training set and test set.\n",
    "\n",
    " \n",
    ">**Exercise:** Download the dataset and place the train and test set in the `datasets/cat_vs_dog` folder. If you're cloning this from github, you should have it in `../datasets/`. So first create the `cat_vs_dog` folder in `datasets` and verify that the data are there by running `ls ../datasets/cat_vs_dog` (or your custom path if you changed it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'datasets/cat_vs_dog'\n",
    "# for train -> apply more changes - to be more chaotic to extrapolate knowledge better\n",
    "# we make sure it identifies objects for what they are and not just because of the image.\n",
    "train_transforms = transforms.Compose([transforms.Resize(255), # size doesnt matter, just has to be consistent\n",
    "                                       transforms.RandomRotation(30), # 30 is the standard\n",
    "                                       transforms.RandomResizedCrop(224), # must be smaller than Resize()\n",
    "                                       transforms.RandomHorizontalFlip(), # enhances the data\n",
    "                                                                          # we need to make sure it extrapolates the data\n",
    "                                       transforms.ToTensor(), # needed for the model to work\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])]) # [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "# on test, the images have to be consistent and not modified in any way.\n",
    "# no randomness on test.\n",
    "test_transforms = transforms.Compose([transforms.Resize(255), # same as in the train part\n",
    "                                      transforms.CenterCrop(224), # same value as in the train part\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225]) ])\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(root_dir + '/train', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(root_dir + '/test', transform=test_transforms)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-lodging",
   "metadata": {},
   "source": [
    "Great! Now that you have downloaded your data, you need to define the transformations to be passed to the `ImageFolder` function. You have already used them with the MNIST dataset (see the Data Augmentation workbook in Pytorch). While for MNIST you were passing the transformation in the following line of code\n",
    "\n",
    "`datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)`\n",
    "\n",
    "here you do it in the `ImageFolder` method. You can think it as a way to work on every dataset, not only on the MNIST one.\n",
    "\n",
    "\n",
    "### Transforms\n",
    "\n",
    "When you load in the data with `ImageFolder`, you'll need to define some transforms. For example, the images are different sizes but we'll need them to all be the same size for training. You can either resize them with `transforms.Resize()` or crop with `transforms.CenterCrop()`, `transforms.RandomResizedCrop()`, etc. We'll also need to convert the images to PyTorch tensors with `transforms.ToTensor()`. Typically you'll combine these transforms into a pipeline with `transforms.Compose()`, which accepts a list of transforms and runs them in sequence. \n",
    "\n",
    "As in the other notebook, you can use the following transformations:\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                                                            [0.5, 0.5, 0.5])])\n",
    "\n",
    "```\n",
    "\n",
    "**WARNING!** Remember that transformation are super useful for \"augmenting\" your training data, so that you make your network less vulnerable to different sizes, rotations, or cropping. However, when you are on the test data, there is no need of augmenting the data! Actually, it is not a good practice to do that because there would be very repetitive test data that invalidates your score.\n",
    "\n",
    "For this reason, define two different transformations for training and test data (remember that `ToTensor()` and normalization are necessary also for the test data, as well as the resizing (you can use `transforms.Resize(size)` for it):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c047d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "likely-cleaners",
   "metadata": {},
   "source": [
    "Now that you have you have defined the needed transformation, it's time to build the Data loader itself!\n",
    "\n",
    "### Data Loaders\n",
    "\n",
    "With the `ImageFolder` loaded, you have to pass it to a [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). The `DataLoader` takes a dataset (such as you would get from `ImageFolder`) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.\n",
    "\n",
    "```python\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "Here `dataloader` is a [generator](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/). To get data out of it, you need to loop through it or convert it to an iterator and call `next()`.\n",
    "\n",
    "```python\n",
    "# Looping through it, get a batch on each loop \n",
    "for images, labels in dataloader:\n",
    "    pass\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(dataloader))\n",
    "```\n",
    " \n",
    ">**Exercise:** Build the dataloader for both the train and test data. Choose the batch size that fits your memory. \n",
    "**Remember** NOT TO shuffle the test data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Run this to test your data loaders\n",
    "images, labels = next(iter(trainloader))\n",
    "imshow(images[0], normalize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-ownership",
   "metadata": {},
   "source": [
    "Ok, now let's create a simple Convolutional Neural Network for this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b19740",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-folks",
   "metadata": {},
   "source": [
    "**N.B.** When building a convolutional neural network, be careful for making all the shapes to match (with Pytorch, in Keras is handled automatically).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "        self.fc1 = nn.Linear(53*53*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc480836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
    "        self.fc1 = nn.Linear(4*4*64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallNet()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 10\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "\n",
    "\n",
    "\n",
    "# best_acc = 0\n",
    "\n",
    "# for e in range(epochs):\n",
    "#     loss_train, loss_test = 0, 0\n",
    "    \n",
    "#     net.train()\n",
    "#     for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output = net.forward(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss_train+=loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     net.eval()\n",
    "#     with torch.no_grad():\n",
    "#         correct, total = 0, 0\n",
    "\n",
    "#         for i, (images, labels) in enumerate(iter(testloader)):\n",
    "\n",
    "#             test_output = net(images)\n",
    "#             loss = criterion(test_output, labels)\n",
    "#             loss_test += loss\n",
    "\n",
    "#             pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "#             correct += (pred_y == labels).float().sum()\n",
    "#             total += len(labels)\n",
    "#             accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "#     acc = (correct/total).float()*100\n",
    "#     print(f'epoch {e+1} done: accuracy of {acc:.2f}, train_loss:{loss_train/len(trainloader.dataset)} and test_loss: {loss_test/len(testloader.dataset)}')\n",
    "\n",
    "#     net.train()\n",
    "#     if e == 0:\n",
    "#         diff_loss = 100\n",
    "\n",
    "#     if abs(loss_test-loss_train) < diff_loss+diff_loss/90 and acc>best_acc:\n",
    "#         diff_loss = abs(loss_test-loss_train)\n",
    "#         best_acc = acc\n",
    "#         torch.save(net.state_dict(), 'model.pth')\n",
    "#         epoch_best = e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-giant",
   "metadata": {},
   "source": [
    "Why do we reshape in the middle of the forward pass?\n",
    "\n",
    "As you remember from the class, the output of a convolutional layer is always a 3D volume! For this reason, since the output channels of the conv2 layer is 16 and the feature maps have size 5x5, then the input of the fc1 layer must be reshaped to have shape `(batch_size, 16 * 5 * 5)`.\n",
    "\n",
    "Now the question is: what will be the difference in the training of this network with respect to the fully-connected one you are used to?\n",
    "\n",
    "None, except for the fact that you do not reshape the input to be a vector, but you keep the shape as a volume! \n",
    "\n",
    ">**Exercise:** Implement a Convolutional Neural Network for the cat vs dog challenge, such that:\n",
    "> - The input images have shape 28x28 and three RGB channels\n",
    "> - You have 2 Conv2d layer with MaxPool2D in the middle and two fully-connected layer at the end.\n",
    "> - You can decide yourself the rest of the hyperparameters (kernel size, number of filters...)\n",
    "> - Train and evaluate your model\n",
    "\n",
    "Following there's an helper function to visualize your prediction once your model has been built.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = train_data.classes\n",
    "\n",
    "def view_classify_general(img, ps, class_list):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    imshow(img, ax=ax1, normalize=True)\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(len(class_list)), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(len(class_list)))\n",
    "    ax2.set_yticklabels([x for x in class_list], size='small')\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "img, label = images[0], labels[0]\n",
    "# Flatten images\n",
    "# Forward pass, get our logits\n",
    "logits = net(img.view(1, *images[0].shape))\n",
    "# Calculate the loss with the logits and the labels\n",
    "ps = torch.exp(logits)\n",
    "    \n",
    "view_classify_general(img, ps, class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-cylinder",
   "metadata": {},
   "source": [
    "You should get something cute like this:\n",
    "\n",
    "![image](imgs/cat_pred.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
