{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carpineti C., Lomonaco V., Bedogni L., Di Felice M., Bononi L., \"Custom Dual Transportation Mode Detection by Smartphone Devices Exploiting Sensor Diversity\", in Proceedings of the 14th Workshop on Context and Activity Modeling and Recognition (IEEE COMOREA 2018), Athens, Greece, March 19-23, 2018\n",
    "#### https://tempesta.cs.unibo.it/projects/us-tm2017/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. All imports in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas     as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import optuna\n",
    "\n",
    "from sklearn.compose         import ColumnTransformer\n",
    "from sklearn.impute          import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics         import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.pipeline        import Pipeline, make_pipeline\n",
    "from lightgbm                import LGBMClassifier\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Analisys and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_5secondWindow.csv')\n",
    "data = data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.describe()\n",
    "# data.head(15)\n",
    "# data.tail(15)\n",
    "# data.isnull().sum()\n",
    "# data.groupby('target').size()\n",
    "# data.groupby('user').size().sort_values()\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.corr()['target'].sort_values().plot.barh(figsize=(45,35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data['target'] != 'Bus']\n",
    "# data = data[data['target'] != 'Train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First split for training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    \n",
    "    # users = data.user.unique().tolist()\n",
    "    # users.sort()    \n",
    "    # train_users = users[:-3]\n",
    "    # test_users = users[-3:]\n",
    "\n",
    "    train_users = ['U1', 'U3', 'U4', 'U5', 'U6', 'U7', 'U8', 'U10', 'U11', 'U13']\n",
    "    test_users = ['U2', 'U9', 'U12']\n",
    "\n",
    "    train_data = data[data['user'].isin(train_users)]\n",
    "    test_data = data[data['user'].isin(test_users)]\n",
    "    \n",
    "    return train_data,test_data\n",
    "\n",
    "df_train, df_test = split_data(data)\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "# df_train.to_csv('dataset_5secondWindow_TRAIN.csv', index=False)\n",
    "# df_test.to_csv('dataset_5secondWindow_TEST.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.groupby('user').size(), df_test.groupby('user').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split test data for individual users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(data):\n",
    "\n",
    "#     user1 = ['U9']\n",
    "#     user2 = ['U2']\n",
    "#     user3 = ['U12']\n",
    "\n",
    "#     test_data1 = data[data['user'].isin(user1)]\n",
    "#     test_data2 = data[data['user'].isin(user2)]\n",
    "#     test_data3 = data[data['user'].isin(user3)]\n",
    "\n",
    "#     return test_data1, test_data2, test_data3\n",
    "\n",
    "# user1, user2, user3 = split_data(df)\n",
    "# user1.shape, user2.shape, user3.shape\n",
    "\n",
    "# user1.to_csv('test_user1.csv', index=False)\n",
    "# user2.to_csv('test_user2.csv', index=False)\n",
    "# user3.to_csv('test_user3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop irrelevant features. \n",
    "#### Drop features with mostly missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['activityrecognition#0', 'time', 'id', 'user'], axis=1)\n",
    "df_train.columns = df_train.columns.str.replace('android.sensor.', '').str.replace('#', '_') # make features more readable\n",
    "df_train_null = (df_train.isnull().sum() / len(df_train)).sort_values(ascending = False)\n",
    "df_train_null = df_train_null.index[df_train_null >= 0.75]\n",
    "\n",
    "df_test = df_test.drop(['activityrecognition#0', 'time', 'id', 'user'], axis=1)\n",
    "df_test.columns = df_test.columns.str.replace('android.sensor.', '').str.replace('#', '_')\n",
    "df_test_null = (df_test.isnull().sum() / len(df_test)).sort_values(ascending = False)\n",
    "df_test_null = df_test_null.index[df_test_null >= 0.75]\n",
    "\n",
    "all_null = list(set(set(df_train_null) | set(df_test_null)))\n",
    "\n",
    "df_train = df_train.drop(all_null, axis=1)\n",
    "df_test  = df_test.drop(all_null, axis=1)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropped Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_null\n",
    "# data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.columns = data.columns.str.replace('android.sensor.', '').str.replace('#', '_')\n",
    "\n",
    "# droped_indices = [data.columns.get_loc(a) for a in all_null]\n",
    "# droped_indices.sort()\n",
    "\n",
    "# droped_indices_new = droped_indices + [0, 2, 68, 69] # 0 id, 2 activityrecognition#0, 68 target, 69 user\n",
    "# droped_indices_new.sort()\n",
    "\n",
    "# droped_indices_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features sets to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. Sensors of first class of classification\n",
    "# cols1 = [c for c in df_train.columns if 'accelerometer' in c or 'sound' in c or 'gyroscope' in c]\n",
    "# df_train1 = df_train[cols1]\n",
    "# df1.columns\n",
    "\n",
    "#### 2. Sensors of second class of classification\n",
    "# cols2 = [c for c in df_train.columns if 'accelerometer' in c or 'sound' in c or 'gyroscope' in c or 'orientation' in c or 'linear_acceleration' in c or 'rotation_vector']\n",
    "# df_train2 = df_train[cols2]\n",
    "# df2.columns\n",
    "\n",
    "#### 3. Sensors of third class of classification\n",
    "# cols3 = [c for c in df_train.columns if 'accelerometer' in c or 'sound' in c or 'gyroscope' in c or 'orientation' in c or 'linear_acceleration' in c or 'rotation_vector' or 'speed' in c]\n",
    "# df_train3 = df_train[cols3]\n",
    "# x3.columns\n",
    "\n",
    "# x1, x2, x3 = df_train1, df_train2, df_train3\n",
    "# features_list = {\"Feature_Set_1\": x1,\n",
    "#                  \"Feature_Set_2\": x2,\n",
    "#                  \"Feature_Set_3\": x3\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['target']\n",
    "x = df_train.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pipeline to include the imputer that handles missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifiers = {\"LightGBM\": LGBMClassifier(boosting_type='gbdt', max_depth=25, n_estimators=129, learning_rate=0.01, min_data_in_leaf=104),\n",
    "                  \"RandomForest\": RandomForestClassifier(criterion='entropy', max_depth=10, max_features=10, min_samples_split=3, n_estimators=300)\n",
    "                  }\n",
    "                  \n",
    "num_4_models = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='median')) ])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[ ('num', num_4_models, x.columns.to_list()) ], remainder='drop')\n",
    "\n",
    "my_classifiers_copy = {name: make_pipeline(preprocessor, model) for name, model in my_classifiers.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train selected models and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f_names, x in features_list.items():\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, random_state=0, test_size=0.2, stratify=y)\n",
    "\n",
    "results = pd.DataFrame({'Model': [], 'Accuracy': [], 'Bal Acc.': [], 'Time': []})\n",
    "\n",
    "for model_name, model in my_classifiers_copy.items():\n",
    "        \n",
    "        start_time = time()\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        predictions = model.predict(x_val)\n",
    "\n",
    "        total_time = time() - start_time\n",
    "\n",
    "        print(f'Finished {model_name} in {round(total_time, 2)} seconds')\n",
    "        \n",
    "        results = results.append({\"Model\": model_name,\n",
    "                                  \"Accuracy\": accuracy_score(y_val, predictions)*100,\n",
    "                                  \"Bal Acc.\": balanced_accuracy_score(y_val, predictions)*100,\n",
    "                                  \"Time\": total_time},\n",
    "                                  ignore_index=True)\n",
    "                                \n",
    "results_ord = pd.DataFrame(results)\n",
    "results_ord = results_ord.sort_values(by=['Accuracy'], ascending=False, ignore_index=True)\n",
    "print(results_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = df_test['target']\n",
    "xx = df_test.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in my_classifiers_copy.items():\n",
    "        \n",
    "        start_time = time()\n",
    "\n",
    "        preds = model.predict(xx)\n",
    "\n",
    "        total_time = time() - start_time\n",
    "\n",
    "        print(f'Finished {model_name} in {round(total_time, 2)} seconds, with an accuracy of: {accuracy_score(preds, yy)*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [1, 3, 5, 7, 10], \"max_features\": [1, 3, 5, 7, 10], \"n_estimators\": [50, 100, 200, 300, 500], \"min_samples_split\": [1, 3, 5, 7, 10]}\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc_grid = GridSearchCV(rfc, rfc_params, cv=10, n_jobs=-1, verbose=2)\n",
    "\n",
    "rfc_grid.fit(preprocessor.transform(x_train), y_train)\n",
    "print(\"Best params: \" + str(rfc_grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    boosting_type = trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"rf\"])\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 32)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
    "    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 10, 500)\n",
    "    \n",
    "    lg_lgbm = LGBMClassifier(boosting_type=boosting_type, max_depth=max_depth, \n",
    "                             n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             min_data_in_leaf=min_data_in_leaf)\n",
    "\n",
    "    score = cross_val_score(lg_lgbm, x, y, n_jobs=-1, cv=2)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save Best Performing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LGBMClassifier(boosting_type='gbdt', max_depth=25, n_estimators=129, learning_rate=0.01, min_data_in_leaf=104)\n",
    "\n",
    "best_model_pipe = make_pipeline(preprocessor, best_model)\n",
    "\n",
    "best_model_pipe.fit(x, y)\n",
    "joblib.dump(best_model_pipe, 'model_lgb78.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6e128dd2cf9bb12ecf94668e33e1546608d302a4f44df6f6afa2b8f73a532e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Strive': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
